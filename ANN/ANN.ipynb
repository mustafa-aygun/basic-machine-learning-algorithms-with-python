{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required modules.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20c0f755270>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fix the randomness.\n",
    "seed = 1234\n",
    "torch.manual_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Download the dataset, and split it into Train, Val, and Test sets.\n",
    "\n",
    "train_transform = T. Compose ([\n",
    "# can add additional transforms on images\n",
    "T.ToTensor () , # convert images to PyTorch tensors\n",
    "T.Grayscale () , # RGB to grayscale\n",
    "T.Normalize ( mean =(0.5 ,) , std=(0.5 ,)) # normalization\n",
    "# speeds up the convergence\n",
    "# and improves the accuracy\n",
    "])\n",
    "val_transform = test_transform = T. Compose ([\n",
    "T.ToTensor () ,\n",
    "T.Grayscale () ,\n",
    "T.Normalize ( mean =(0.5 ,) , std=(0.5 ,))\n",
    "])\n",
    "\n",
    "train_set = CIFAR10(root=\"CIFAR10\",train=True,transform = train_transform, download=True) #Get train data set.\n",
    "train_set_length = int (0.8 * len(train_set)) #Divide train set as train and validation.\n",
    "val_set_length = len(train_set)-train_set_length\n",
    "\n",
    "train_set, val_set = random_split(train_set,[train_set_length,val_set_length]) #Randomly split train and validation data sets.\n",
    "test_set = CIFAR10(root=\"CIFAR10\", train=False,transform=test_transform, download=True) #Get test data set.\n",
    "#Define the data loaders.\n",
    "batch_size = 1024\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_set,batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the ANN\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,neuron,activation_function,layer) -> None: # Getting inputs.\n",
    "        super().__init__()\n",
    "        self.activation_function = activation_function # Giving activation function as parameter.\n",
    "        self.layer = layer # Giving layer as parameter too.\n",
    "        if self.layer == 1: # Because of I tried 3 layer, I just build a quick setup with if elses. \n",
    "            self.layer1 = nn.Linear(in_features=32*32,out_features=neuron) # If I have 1 layer between input and output, I am directly giving the neuron.\n",
    "            self.layer2 = nn.Linear(in_features=neuron,out_features=10)\n",
    "        elif self.layer == 2:\n",
    "            self.layer1 = nn.Linear(in_features=32*32,out_features=neuron*2) # If I have 2 layer between input and output, I am multplying it with 2 and 1.\n",
    "            self.layer2 = nn.Linear(in_features=neuron*2,out_features=neuron*1) # For example, if neuron is 100, It becomes 1024->200, 200->100, 100->10\n",
    "            self.layer3 = nn.Linear(in_features=neuron*1,out_features=10)\n",
    "        elif self.layer == 3:\n",
    "            self.layer1 = nn.Linear(in_features=32*32,out_features=neuron*3) # If I have 3 layer between input and output, I am multplying it with 3, 2 ,and 1.\n",
    "            self.layer2 = nn.Linear(in_features=neuron*3,out_features=neuron*2) # For example, if neuron is 100, It becomes 1024->300, 300->200, 200->100, 100->10.\n",
    "            self.layer3 = nn.Linear(in_features=neuron*2,out_features=neuron*1)\n",
    "            self.layer4 = nn.Linear(in_features=neuron*1,out_features=10) # In every case it has to finish 10.\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,1)\n",
    "        if self.layer == 1: # As above, I just build basic if else setup for acivation functions. According to number of layer, it adds activation function between them.\n",
    "            x = self.layer1(x) # Because of my Python skills not good enough, I prefer building this setup basically and focusing ML part more.\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer2(x)\n",
    "        elif self.layer == 2:\n",
    "            x = self.layer1(x)\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer3(x)\n",
    "        elif self.layer == 3:\n",
    "            x = self.layer1(x)\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.activation_function(x)\n",
    "            x = self.layer4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-1,1e-3,1e-4] \n",
    "neurons = [100,150]\n",
    "activation_functions = [nn.ReLU(),nn.Sigmoid()]\n",
    "layers = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model and train it for 100 epochs\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "num_epochs = 100 # Number of Epoch\n",
    "modelID = 0 # Initializing model id\n",
    "results = []\n",
    "for learning_rate in learning_rates:\n",
    "    for neuron in neurons:\n",
    "        for activation_function in activation_functions:\n",
    "            for layer in layers:\n",
    "                model = MyModel(neuron=neuron,activation_function=activation_function,layer=layer).to(device)\n",
    "                best_model = model\n",
    "                loss_function = nn.CrossEntropyLoss()\n",
    "                optimizer =  torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "                last_validation_accuracy = 0 # Giving initally low value to replace it quickly.\n",
    "                last_val_loss = 100 # Giving initally high value to replace it quickly.\n",
    "                patience = 10 # Defining patience\n",
    "                print(f'ModelID: {modelID}| Learning_Rate = {learning_rate}\\tNeuron = {neuron}\\tActivation_function = {activation_function}\\tLayer = {layer}')      \n",
    "                for epoch in tqdm(range(num_epochs)):\n",
    "                #Training\n",
    "                    model.train()\n",
    "                    accum_train_loss = 0\n",
    "                    for i, (imgs,labels) in enumerate(train_loader,start=1):\n",
    "                        imgs, labels = imgs.to(device), labels.to(device)\n",
    "                        output = model(imgs)\n",
    "                        loss = loss_function(output, labels)\n",
    "\n",
    "                        #Accumlate the loss\n",
    "                        accum_train_loss += loss.item()\n",
    "\n",
    "                        #Backpropagation\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    #Validation\n",
    "                    model.eval()\n",
    "                    accum_val_loss = 0\n",
    "                    correct2 = total2 = 0 # Initializing values to calculate validation accuracy.\n",
    "                    with torch.no_grad():\n",
    "                        for j, (imgs,labels) in enumerate(val_loader, start=1):\n",
    "                            imgs,labels = imgs.to(device),labels.to(device)\n",
    "                            output = model(imgs)\n",
    "                            _, predicted_labels = torch.max(output,1)\n",
    "                            correct2 += (predicted_labels == labels).sum()\n",
    "                            total2 += labels.size(0)\n",
    "                            accum_val_loss += loss_function(output,labels).item()\n",
    "                    validation_accuracy = 100 * correct2/total2 # Calculate validation accuracy like in test accuracy.\n",
    "                    print(f'Validation Accuracy = {validation_accuracy:.3f}%')\n",
    "                    #Print statistics of the epoch\n",
    "                    print(f'Epoch = {epoch} | Train Loss = {accum_train_loss / i:.4f}\\tVal Loss = {accum_val_loss / j:.4f}')\n",
    "\n",
    "                    val_loss = accum_val_loss / j # Getting val_loss for this epoch. \n",
    "                    train_loss = accum_train_loss / i # Getting train_loss for this epoch. \n",
    "\n",
    "                    if epoch == 0: # For the first epoch, we are definning our first result as our best result.\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_val_loss = val_loss\n",
    "                        best_train_loss = train_loss\n",
    "                        best_values = [learning_rate, neuron, activation_function,layer, epoch, best_validation_accuracy,best_val_loss,best_train_loss,modelID]\n",
    "\n",
    "                    if validation_accuracy > last_validation_accuracy and val_loss> last_val_loss: \n",
    "                        patience -= 1 # If validation accuracy and validation loss higher than previous one loss patience\n",
    "                    elif validation_accuracy > best_validation_accuracy and val_loss  < best_val_loss:\n",
    "                        best_model = model # If validation accuracy is better and validation loss is lower, it means it is our new best model.\n",
    "                        best_validation_accuracy = validation_accuracy\n",
    "                        best_val_loss = val_loss # Giving values to compare them later.\n",
    "                        best_train_loss = train_loss # Save the result.\n",
    "                        best_values = [learning_rate, neuron, activation_function,layer, epoch, best_validation_accuracy,best_val_loss,best_train_loss,modelID]\n",
    "                    \n",
    "                    print(f'Patience Left = {patience}')\n",
    "                    if patience == 0: # If patience is 0, save the best model to a local path.\n",
    "                        save_path = f\"./bestmodels/learning_rate{learning_rate}/neuron{neuron}/activation_function{activation_function}/layer{layer}\"\n",
    "                        os.makedirs(save_path, exist_ok=True)\n",
    "                        torch.save(best_model.state_dict(),f\"{save_path}/model.pth\")\n",
    "                        results.append(best_values) # Append our best values to a list\n",
    "                        break\n",
    "                    if num_epochs - 1 == epoch: # If number of epoch finishes, save the best model to a local path. \n",
    "                        save_path = f\"./bestmodels/learning_rate{learning_rate}/neuron{neuron}/activation_function{activation_function}/layer{layer}\"\n",
    "                        os.makedirs(save_path, exist_ok=True)\n",
    "                        torch.save(best_model.state_dict(),f\"{save_path}/model.pth\")\n",
    "                        results.append(best_values)  # Append our best values to a list\n",
    "                    last_val_loss = val_loss # Giving values to another variable to compare them in next run. \n",
    "                    last_validation_accuracy = validation_accuracy\n",
    "                print(best_values)\n",
    "                modelID += 1 # Increase model id.\n",
    "file = open('train_results.csv', 'w+', newline ='') # Print all results to a excel file.\n",
    "with file:     \n",
    "    write = csv.writer(file) \n",
    "    write.writerows(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the test accuracy \n",
    "test_results = []\n",
    "modelID = 0 # Initializing model id\n",
    "for learning_rate in learning_rates:\n",
    "    for neuron in neurons:\n",
    "        for activation_function in activation_functions:\n",
    "            for layer in layers:\n",
    "                from_path = f\"./bestmodels/learning_rate{learning_rate}/neuron{neuron}/activation_function{activation_function}/layer{layer}/model.pth\"\n",
    "                print(f'Learning_Rate = {learning_rate}\\tNeuron = {neuron}\\tActivation_function = {activation_function}\\tLayer = {layer}')\n",
    "                print(f\"./bestmodels/learning_rate{learning_rate}/neuron{neuron}/activation_function{activation_function}/layer{layer}/model.pth\")\n",
    "                model = MyModel(neuron=neuron,activation_function=activation_function,layer=layer).to(device) # Creating a model\n",
    "\n",
    "                model.load_state_dict(torch.load(from_path)) # Loading our best models from the local path.\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    correct = total = 0\n",
    "                    for images, labels in test_loader:\n",
    "                        images, labels = images.to(device),labels.to(device)\n",
    "                        output = model(images)\n",
    "                        _, predicted_labels = torch.max(output,1)\n",
    "                        correct += (predicted_labels == labels).sum()\n",
    "                        total += labels.size(0)\n",
    "                print(f'Test Accuracy = {100 * correct/total:.3f}%')\n",
    "                test_result = [modelID,100 * correct/total]\n",
    "                test_results.append(test_result) # Getting test results in the list.\n",
    "                modelID += 1\n",
    "\n",
    "file = open('test_results.csv', 'w+', newline ='') \n",
    "with file:     # Printing results to an excel file.\n",
    "    write = csv.writer(file) \n",
    "    write.writerows(test_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8998e62e66f690298e570e12272f0dfb8270c94f71cc43675c5022fdef293b71"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
